models:
  next_purchase:
    params:
      activation: relu
      batch_size: 1
      epochs: 50
      l1: 0.01
      l2: 0.01
      lag: 4
      lahead: 4
      lr: 0.001
      split_ratio: 0.8
      tsteps: 1
      units: 128
    hyper_params:
      activation: relu_tanh
      batch_size:
        - 5120
        - 10240
        - 20480
      epochs:  1000*40000
      split_ratio: 0.8
      filters:
        - 1
        - 2
      l1:
        - 0.0001
      l2:
        - 0.0001
      lr:
        - 0.001
      kernel_size:
        - 1
        - 2
      max_pooling_unit:
        - 1
        - 2
      lstm_units:
        - 16
        - 32
      units:
        - 8
        - 16
        - 32
        - 64
      loss: mae
      drop_out_ratio': 0.1*0.5
  purchase_amount:
    params:
      activation: relu
      batch_size: 32
      epochs: 40
      l1: 0.0001
      l2: 0.0001
      lr: 0.001
      split_ratio: 0.8
      filters: 2
      kernel_size: 4
      max_pooling_unit: 2
      lstm_units: 32,
      units: 8
      loss: mae
      drop_out_ratio: 0.1
      num_layers: 1
    hyper_params:
      activation: relu_tanh_sigmoid_softmax
      batch_size: 16_32_64_128_256_512_1024
      epochs: 5*40
      split_ratio: 0.8_0.9
      filters: 1_2
      l1: 0.001_0.0001_0.00001
      l2: 0.001_0.0001_0.00001
      lr: 0.1_0.01_0.0001*0.5_0.05_0.0005
      kernel_size: 2*32
      max_pooling_unit': 1*10
      lstm_units': 6_32_64_128_256_512_1024
      units': 6_32_64_128_256_512_1024,
      loss': mae,
      num_layers':
        min: 1
        max: 3
      drop_out_ratio: 0.1*0.5





