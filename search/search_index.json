{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Customer Lifetime Value Prediction","text":"<p>This framework we generate 2 main predictive model per customer.  First, Next Purchase (Frequency) Model will be trained.  This model will help us to predict the day of nex purchases per customer Second, Customer Value Model will be trained.  THis model will help us to predict what will be the amount of next purchases per customer. There will be customers can not be predicted by those models above because of lack historical informations.  Those customers are NewComers. This platform allows us to predict NewComers' total lifetime values as well.</p>"},{"location":"#installation","title":"Installation","text":"<p>Tool can be used any other package by install it via pypi or git command</p> <p><pre><code>poetry add clv_prediction\n</code></pre> OR</p> <pre><code>poetry add git+https://github.com/caglanakpinar/clv_prediction.git\n</code></pre>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>clv/\n    docs/   \n        - configs.yaml\n        - test_parameters.yaml\n    confgis.py\n    dashboard.py\n    data_access.py\n    executor.py\n    functions.py\n    main.py\n    newcomers.py\n    next_purchase_model.py\n    next_purchase_prediction.py\n    purchase_amount_model.py\n    utils.py\n</code></pre>"},{"location":"data_access/","title":"Data Access","text":"<p>Here is the data source that you can connect with your SQL queries:</p> <ul> <li>Ms SQL Server</li> <li>PostgreSQL</li> <li>AWS RedShift</li> <li>Google BigQuery</li> <li>.csv</li> <li>.json</li> <li>pickle</li> <li>parquet</li> </ul>"},{"location":"data_access/#connection-postgresql-ms-sql-aws-redshift","title":"Connection PostgreSQL - MS SQL - AWS RedShift","text":"<pre><code>    data_source = \"postgresql\"\n    connector = {\"user\": ***, \"password\": ***, \"server\": \"127.0.0.1\",\n                 \"port\": ****, \"db\": ***}\n    data_main_path =\"\"\"\n                       SELECT\n                        customer_indicator,\n                        amount_indicator,\n                        time_indicator,\n                       FROM table\n                   \"\"\"\n</code></pre>"},{"location":"data_access/#connection-google-bigquery","title":"Connection Google BigQuery","text":"<pre><code>    data_source = \"googlebigquery\"\n    connector = {\"data_main_path\": \"./json_file_where_you_stored\",\n                 \"db\": \"flash-clover-*********.json\"}\n    data_main_path =\"\"\"\n               SELECT\n                    customer_indicator,\n                    amount_indicator,\n                    time_indicator,\n                FROM tablee\n           \"\"\"\n</code></pre>"},{"location":"data_access/#connection-csv-json-pickle-parquet","title":"Connection csv - .json - .pickle - .parquet","text":"<pre><code>    data_source = \"csv\"\n    data_main_path = \"./data_where_you_store/***.csv\"\n</code></pre>"},{"location":"data_preprocess/","title":"Data Preprocess","text":"<p>Each model has unique aggregation in order to prepare data to create model.</p>"},{"location":"data_preprocess/#1-next-purchase-model-data-preparation","title":"1. Next Purchase Model Data Preparation","text":"<p>Time difference of each order per customer is calculated as day difference of orders per user. Normalized each time difference values related to Min-Max Normalization Method. Each customer of Min-Max Normalization individually. number of lag parameter is tuned by using ARIMA time series model. Regarding of lahead parameter of LSTM model, data set is shaped per customer. Iteratively each customer of data set is collected individually. the process is parallelized according to CPU count. When model data has been prepared per customer,  it is split according to split ratio into the train and test data set (train_x, train_y, test_x, test_y).</p> customers lag 3 lag 2 lag 1 y user_1 0,4 0,8 1,7 1,2 user_1 0,8 1,7 1,2 1,4 user_1 1,7 1,2 1,4 1,6 user_1 1,2 1,4 1,6 2,8 user_1 1,4 1,6 2,8 2,9 user_1 1,6 2,8 2,9 3,0 user_2 5,4 5,8 5,7 5,2 user_2 5,8 5,7 5,2 5,4 user_2 5,7 5,2 5,4 5,6 user_2 5,2 5,4 5,6 5,8 user_2 5,4 5,6 5,8 5,9 user_2 5,6 5,8 5,9 5,0"},{"location":"data_preprocess/#2-purchase-amount-model-data-preparation","title":"2. Purchase Amount Model Data Preparation","text":"<p><code>order_count</code> parameter refers us for the model of the feature count.  previous orders of purchase amounts of each customer is collected from fow data. Regarding of <code>lahead</code> parameter of LSTM model, data set is shaped per customer. Iteratively each customer of data set is collected individually. the process is parallelized according to CPU count. When model data has been prepared per customer,  it is split according to split ratio into the train and test data set (train_x, train_y, test_x, test_y).</p>"},{"location":"data_preprocess/#why-do-we-need-order-count-as-a-feature-at-purchase-amount-model","title":"Why do we need order count as a feature at Purchase Amount Model?","text":"<p>Order count is also the feature number of the purchase amount model.</p>"},{"location":"data_preprocess/#caution","title":"Caution","text":"<p>Order count must be inserted into the <code>test_parameters.yaml</code> in order not to allow for changing later on prediction.  Once the model is built with calculated <code>order_count</code> it must be predicted with the same order count.</p>"},{"location":"data_preprocess/#why-we-need-an-order-count-of-a-decision","title":"Why we need an order count of a decision?","text":"<p>It is a crucial parameter for the purchase amount model.  The purchase amount is a 1 Dimensional Conv NN. It works with kernel sizes and they are related to feature size. At the purchase amount model features are sequential orders.  For instance if we assign order count as 5, time_period.user_1, user_2, user_3, user_4 have 100, 101, 300, 2 orders. The data set will be;</p> <ul> <li> <p><code>user_1: 95th, 96th, 97th, 98th, 99th, 100th  orders</code></p> </li> <li> <p><code>user_2: 96th, 97th, 98th, 99th, q00th, 101st  orders</code></p> </li> <li> <p><code>user_3: 295th, 296th, 297th, 298th, 299th, 300th  orders</code></p> </li> <li> <p><code>user_4: only have 2 orders first 9 orders will be 0 and this will affect the model process</code></p> </li> </ul> <p>It is now crucial to have a minimum 0 assigned order as user_4 However, it is also a crucial point to get as much previous order count for make kernel size larger. The order count must be optimized even sending to the platform as an argument.  If this argument is not using, the platform handles for deciding order_count***.</p> customers Last 5 Last 4 Last 3 Last 2 Last Order (y) user_1 10,4 13,4 18,4 11,4 15,4 user_2 50,8 52,8 54,8 56,8 58,8 user_3 30,7 25,7 15,7 10,7 8,7 user_4 20,2 23.5 26,2 27,2 29,2 user_5 1,4 1,4 1,4 1,4 1,4 user_6 12,6 30,6 12,6 30,6 12,6"},{"location":"data_preprocess/#3-newcomers-model-data-preparation","title":"3. NewComers Model Data Preparation","text":"<p><code>order_count</code> argument will be used in order to assign users weather NewComer pr not.  THis parameter can be while initializing the platform as below;</p> <pre><code>    from clv.executor import CLV\n    order_count = 3  # users who have &lt; 3 orders will be new comers\n    clv = CLV(customer_indicator=customer_indicator,\n              ...\n              order_count: int | None, \n              ....\n\n    )\n</code></pre> <p>If <code>order_count=None</code>, <code>feature_count</code> in <code>test_parameters.yaml</code> will be used.</p>"},{"location":"data_preprocess/#why-do-we-need-order-count-as-a-feature-at-newcomers-clv-model","title":"Why do we need order count as a feature at NewComers CLV Model?","text":"<p>It is a crucial parameter for NewComers Model;</p> <ul> <li> <p>Users who have an order count less than <code>order_count</code> are not included in Combined of Next Purchase - Purchase Amount Models.</p> </li> <li> <p>NewComers are individually predicted according to a dependent value is <code>order_count</code>.</p> </li> </ul> <p>Main concept of Newcomers is for predicting orders count daily. Feature value is going to be total number of order count for all NewComers. Normalized each order count values related to Min-Max Normalization Method per day. Regarding of <code>lahead</code> parameter of LSTM model, data set is shaped just like below;</p> days lag 3 lag 2 lag 1 y  (total order count of Newcomers) 2021-05-01 25 5 10 20 2021-05-02 5 10 20 30 2021-05-03 10 20 30 40 2021-05-04 20 30 40 60 2021-05-05 30 40 60 70 2021-05-06 40 60 70 90 2021-05-07 60 70 90 100 <p>When model data has been prepared per day just like above. it is split according to split ratio into the train and test data set (train_x, train_y, test_x, test_y). LSTM model will be implemented for the data just like above.</p> <p>Prediction process is calculated sequentially per day. Each day model has been regenerated (store .json format)  with updated coefficient matrix (stored as .h5 format). LSTM allows us to predict next step values regarding your lags. While we are predicting further prediction, models of coefficients must be updated and the previous prediction values must be merged just like actual values. But, just like the recent prediction and model, the Tuned parameters are will also be used for further future days of predictions.</p>"},{"location":"intro/","title":"welcome to clv-prediction 101","text":"<p>Welcome to clv-prediction world.  This platform allows you to run customers lifetime prediction for future relationship with a customer. You can connect to any data source and run CLV without touching any code all you need to pass arguments like user ID etc.</p>"},{"location":"intro/#what-is-this-all-about","title":"what is this all about?","text":"<p>This is about to build a pipeline which is starting from fetching data, build model for users next purchases/sessions in platform, make prediction per customer based on built model from previous step. It also treats individually for new users who have no purchase or sessions background on historical data. Finally, when all CLV train/prediction process have been completed, a dashboard will be available to visualize.</p>"},{"location":"intro/#why-do-need-clv-prediciton","title":"Why do need <code>clv-prediciton</code>?","text":"<p>recent years, companies needed to see their feature and relation with their customers.  This will help them to act proactively for the upcoming trends of users engagements. For instance, Users might return churn in feature, so clv-prediction will give overview churn users. Here are the benefits to use clv-prediction:  - Allows you to predict your business of customers values individually.  - Predicts customers of next purchase dates.  - Predicts customers of next purchase amounts.  - Predicts newcomers of next purchase amounts.  - Dashboard for visualization predicted values.</p>"},{"location":"intro/#step-by-step-instruction-to-use","title":"Step by Step Instruction to Use","text":"<p>there are 2 sections;  - execute model train  - run dashboard</p>"},{"location":"intro/#how-to-run-clv-and-dashboard","title":"How to run clv and dashboard","text":"<pre><code>    from clv.executor import CLV\n    clv = CLV(customer_indicator=customer_indicator,\n              amount_indicator=amount_indicator,\n              date=date,\n              order_count=order_count,\n              data_source=data_source,\n              data_query_path=data_query_path,\n              time_period=time_period,\n              time_indicator=time_indicator,\n              export_path=export_path,\n              connector=connector)\n    clv.show_dashboard()\n</code></pre>"},{"location":"intro/#how-it-works","title":"How it works?","text":""},{"location":"intro/#main-concept","title":"Main Concept","text":"<p>This framework we generate 2 main predictive model per customer.  First, Next Purchase (Frequency) Model will be trained.  This model will help us to predict the day of nex purchases per customer Second, Customer Value Model will be trained.  THis model will help us to predict what will be the amount of next purchases per customer. There will be customers can not be predicted by those models above because of lack historical informations.  Those customers are NewComers. This platform allows us to predict NewComers' total lifetime values as well.</p>"},{"location":"intro/#prediction-of-next-purchase-frequency-per-customer-model","title":"Prediction of Next Purchase (Frequency) per Customer Model","text":"<p>Each customer of historical purchases of date differences is calculated.     There will be accepted patterns related to customers ' behaviors.     Some Users might have a pattern of every Monday.     Some will have Mondays -Wednesdays- Fridays.     There must be an individual predictive model for each customer, and this must be the Time Series model per each customer of historical frequency.     However, it is not an efficient way and there will be a computational cost here. In that case, Deep Learning can handle this problem with LSTM NN (check next_purchase_model.py).     There must be a model that each customer of frequency values are able to be predicted.</p>"},{"location":"intro/#prediction-of-customer-value-value-per-customer-model","title":"Prediction Of Customer Value (Value) per Customer Model","text":"<p>Customer future values of prediction are also crucial to reach the final CLV calculation.     Once frequency values are calculated per customer, by historical users' of purchase values can be predicted via using Deep Learning.     At this process, there is a built-in network (check purchase_amount.py) which is created by using 1 Dimensional Convolutional LSTM NN.</p>"},{"location":"intro/#prediction-of-newcomers-clv-model","title":"Prediction Of NewComers CLV Model","text":"<p>Newcomers are not likely predictable as Engaged users.  They probably not have stabilized transactions pattern or they will not have a fitted train model unless they have enough transactions.     At this point, rather than predicting the value of each transaction, predicting the amount of transaction will be more convenient.     By using the historical total purchases per time period (daily), the next time period of total purchase count is able to be predicted.     Assuming that Purchase Amount of Newcomers are Normal Distributed (Hypothesis Test).     In that case, purchase Amount prediction per newcomer is going to be the Mean of Purchase Amounts.</p>"},{"location":"intro/#combining-of-next-purchase-model-purchase-amount-prediction-model-newcomers-prediction-model","title":"Combining Of Next Purchase Model &amp; Purchase Amount Prediction Model &amp; NewComers Prediction Model","text":"<p>Without predicting the frequency of users, we can not be sure when the customer will have a purchase.     So, by using the next purchase model, customers of future purchase dates have to be predicted.     Before predicting a date, the algorithm makes sure the predicted future order of dates is in selected time period.</p> <p>last purchased date from raw data &lt; predicted purchase date &lt; last purchased date from raw data + time period</p> <p>This time period must be assigned when the process is initialized.  The time period will have a range between the last transaction date of the dataset and the last transaction date + time period. It can be detected the users' purchases of dates and the next process will be predicting each purchase of values by using the Purchase Amount model.</p> <p>After combining Of Next Purchase Model &amp; Purchase Amount Prediction Model is done, NewComers of Predictions are merging the results.</p>"},{"location":"intro/#clv-prediction-process-pipeline","title":"CLV Prediction Process Pipeline","text":""},{"location":"monitoring/","title":"Dashboard for CLV Prediction","text":"<p>Here are examples of dashboard</p> <p></p> <p></p>"},{"location":"monitoring/#how-does-it-work","title":"How does it work?","text":"<pre><code>    from clv.executor import CLV\n    clv = CLV(customer_indicator=customer_indicator,\n              amount_indicator=amount_indicator,\n              date=date,\n              order_count=order_count,\n              data_source=data_source,\n              data_query_path=data_query_path,\n              time_period=time_period,\n              time_indicator=time_indicator,\n              export_path=export_path,\n              connector=connector)\n    clv.show_dashboard()\n</code></pre>"},{"location":"monitoring/#dashboard-of-components","title":"Dashboard of Components","text":""},{"location":"monitoring/#1-clv-prediction-time-line","title":"1. CLV Prediction Time Line","text":"<p>Related to result_data.csv file, all previously calculated results are combined and showed in the line chart.</p> <p></p>"},{"location":"monitoring/#2-churn-customers-of-purchase-timeline","title":"2. Churn Customers Of Purchase TimeLine","text":"<p>According to the selected date from CLV Prediction Time Line,  the customers who have purchased before the selected date but never had an order in prediction time periods are detected. These are the churn customers of the selected date.</p> <p></p>"},{"location":"monitoring/#3-newcomer-customers-of-purchase-timeline","title":"3. Newcomer Customers Of Purchase TimeLine","text":"<p>According to the selected date from CLV Prediction Time Line,  the customers, who are newcomers at the selected date and haven`t purchased before the selected date, are detected. These are the churn customers of the selected date.</p> <p></p>"},{"location":"monitoring/#4-top-100-the-least-engaged-customers-of-sum-values-per-month","title":"4. Top 100 the Least Engaged Customers Of Sum Values per month","text":"<p>The customers who have fewer purchase amounts than others of purchase amounts sum/mean values in the timeline. These customers are able to be selected individually from the filter, Worst Customer List.</p> <p></p>"},{"location":"monitoring/#5-top-100-the-most-engaged-customers-of-sum-values-per-month","title":"5. Top 100 the Most Engaged Customers Of Sum Values per month","text":"<p>The customers who have more purchase amounts than others of purchase amount sum/mean values in the timeline. These customers are able to be selected individually from the filter, Top Customer List.</p> <p></p>"},{"location":"monitoring/#6-churn-rate-and-newcomer-rate-per-month","title":"6. Churn Rate and Newcomer Rate per month","text":"<p>These pie charts refer to Newcomer and Churn Rate of the Business According to selected date in CLV Prediction Timeline.</p> <p></p>"},{"location":"params/","title":"CLV Prediction Parameters","text":""},{"location":"params/#job","title":"job","text":"<p>Train, Prediction, Train &amp; Prediction. when <code>jon = 'train''</code>, steps are going to be the Next Purchase Model training and Purchase Amount Model training. Each model of the hyperparameter tuning process will be initialized before models have been initialized. Once, the hyperparameter has been progressed tuned network parameters  are stored in test_parameters.yaml where it is in <code>export_path</code>. When a model has been run repeatedly (or periodically), the model has been checked whether  it has been already built during the ***time_period. If there are stored models in <code>export_path</code>, the latest model  is imported and move on to the next process without a run for building the model. When the <code>job='prediction'</code>, first, the next purchase per customer is predicted then,  the purchase amount is predicted related to the next purchase prediction. When the <code>job='train_prediction'</code>, first, framework will do <code>jon = 'train''</code>, then, <code>job='prediction'</code></p>"},{"location":"params/#order_count","title":"order_count","text":"<p>It allows us to create a feature set of the purchase amount model. (Check Why do we need order count as a feature at Purchase Amount Model? for details). if it is not assigned (it is not a required argument in order to initialize the clv prediction),  the platform handles it to decide the optimum order count. Order Count also affects the detection of NewComers.</p>"},{"location":"params/#customer_indicator","title":"customer_indicator","text":"<p>This parameter indicates which column represents a unique customer identifier on given data.</p>"},{"location":"params/#amount_indicator","title":"amount_indicator","text":"<p>This parameter indicates which column represents purchase value (integer, float ..) on the given data.</p>"},{"location":"params/#time_indicator","title":"time_indicator","text":"<p>This parameter indicates which column represents order checkout date with date  format (timestamp) (YYYY/MM/DD hhss, YYYY-MM-DD hhss, YYYY-MM-DD) on given data.</p>"},{"location":"params/#date","title":"date","text":"<p>This allows us to query the data with a date filter. This removes data that occurs after the given date. If the date is not assigned there will be no date filtering.  date arguments are filtering related to time_indicator column, make sure it is querying with the accurate format. If clv prediction is running with schedule service, periodically given date is updated and filter with an updated given date. If the date is not assigned when clv prediction is scheduling, the date will be the current date.</p>"},{"location":"params/#data_source","title":"data_source","text":"<p>The location where the data is stored or the query (check data source for details).</p>"},{"location":"params/#data_query_path","title":"data_query_path","text":"<p>Type of data source to import data to the platform (optional Ms SQL, PostgreSQL, AWS RedShift,  Google BigQuery, csv, json, pickle).</p>"},{"location":"params/#connector","title":"connector","text":"<p>if there is a connection parameters as user, password, host port,  this allows us to assign it as dictionary format (e.g {\"user\": , \"pw\": , \"db\"\": ***}).</p>"},{"location":"params/#export_path","title":"export_path","text":"<p>Export path where the outputs are stored, created models (.json format), tuned parameters (test_parameters.yaml), schedule service arguments (schedule_service.yaml),  result data with predicted values per user per predicted order  (.csv format) are willing to store at given path. When prediction is initialized, Nex Purchase Model will create folder  <code>temp_next_purchase_results</code> and Purchase Amount Model will create folder 'temp_purchase_amount_results' in order to import results as .csv format</p>"},{"location":"params/#time_period","title":"time_period","text":"<p>A period of time which is willing to predict.  Supported time periods month, week, '2week', quarter, '6month' (Required). (by default, it is <code>time_period='week'</code>)</p>"},{"location":"params/#time_schedule","title":"time_schedule","text":"<p>A period of time which handles for running clv_prediction train or prediction process periodically.  Supported schedule periods day, year, month, week, 2*week.</p>"},{"location":"train/","title":"CLV Train Models and Prediction","text":""},{"location":"train/#train-job-train","title":"Train  - <code>job = 'train'</code>","text":"<p>Next Purchase Model, Purchase Amount Model and NewComer Model of the train process are progressed via tensorflow - Keras. It is a LSTM NN. Trained model stored at <code>export_path</code> with .json format. .json trained file has a file name with <code>time_period</code>, name of the model, trained date (current date). e.g; trained_purchase_amount_model_20210101_month.json</p> <p>Before initialize the training process previously-stored model are checked which have been stored at <code>export_path</code>     The most recent trained must be picked. Model name and <code>time_period</code>  also must be matched.     e.g; recent model: trained_purchase_amount_model_20210101_month.json, model name: purchase_amount, time_period: month,       current date 2020-01-30. This model trained 29 days before which is accepted range (accepted range 0 - 30 (one month)).</p>"},{"location":"train/#train-prediction-proces-job-train_prediction","title":"Train-Prediction Proces <code>job = 'train_prediction'</code>","text":"<p>Each model process is trained, then they are predicted sequentially.  At the end 3 models have been generalized,  3 models of parameters tuning have been applied and 3 models of predictions are calculated.</p>"},{"location":"train/#running-clv-prediction","title":"Running CLV Prediction","text":"<pre><code>    customer_indicator = \"user_id\"\n    amount_indicator = \"transaction_value\"\n    time_indicator = \"days\"\n    time_period = 'month'\n    job = \"train\" # prediction or train_prediction\n    date = '2021-01-01'\n    order_count = 15\n    data_source = \"postgresql\"\n    data_query_path=\"\"\"\n                select user_id,\n                       transaction_value,\n                       days\n                from purchases\n    \"\"\"\n    export_path =  './data'\n    connector = {\"db\": \"c****s\",\n                 \"password\": \"******\",\n                 \"port\": \"5**3\",\n                 \"server\": \"127.0.0.1\",\n                 \"user\": \"*******\"}\n\n    from clv.executor import CLV\n    clv = CLV(customer_indicator=customer_indicator,\n              amount_indicator=amount_indicator,\n              job=job,\n              date=date,\n              order_count=order_count,\n              data_source=data_source,\n              data_query_path=data_query_path,\n              time_period=time_period,\n              time_indicator=time_indicator,\n              export_path=export_path,\n              connector=connector)\n    clv.clv_prediction()\n</code></pre>"},{"location":"train/#collecting-prediction-result-data","title":"Collecting Prediction Result Data","text":"<p>Once, prediction process has been initialized (<code>job: 'prediction'</code> or <code>'train_prediction'</code>),  It can be collected via <code>get_result_data</code>. This data will be represented with raw data per customer of next purchase orders.</p> <pre><code>    from clv.executor import CLV\n    clv = CLV(customer_indicator=customer_indicator,\n              amount_indicator=amount_indicator,\n              date=date,\n              order_count=order_count,\n              data_source=data_source,\n              data_query_path=data_query_path,\n              time_period=time_period,\n              time_indicator=time_indicator,\n              export_path=export_path,\n              connector=connector)\n    results = clv.get_result_data()\n</code></pre> customers data_type time_indicator amount_indicator user_1 actual 2021-01-07 10,4 user_1 actual 2021-01-14 15,4 user_1 actual 2021-01-28 20,4 user_1 prediction 2021-02-05 25,4 user_1 prediction 2021-02-06 30,8 user_2 prediction 2021-02-05 8,7 user_3 prediction 2021-02-05 29,2 user_4 prediction 2021-02-05 1,4 user_4 prediction 2021-02-06 18,6 newcomers prediction 2021-02-05 12,6 newcomers prediction 2021-02-06 12,6"},{"location":"tune/","title":"HyperParameter Tuning","text":"<p>Parameters of networks (LSTM NN &amp; ! Dimensional Conv NN) are tuned via Keras Turner Library.  However, <code>batch_size</code> and <code>epoch</code> are tuned individually.</p>"},{"location":"tune/#tuning-epoch-and-batch_size","title":"Tuning <code>epoch</code> and <code>batch_size</code>","text":"<p><code>epoch</code> hyperparameters are sorting as ascending and <code>batch_size</code> hyperparameters are sorting as descending. Each iteration sorted parameters are used and loss values are calculated. We aim here to capture the best of the minimum <code>epoch</code> and the best of the maximum <code>batch_size</code>.</p> <p></p>"}]}